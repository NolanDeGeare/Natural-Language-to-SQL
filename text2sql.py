# -*- coding: utf-8 -*-
"""text2sql.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mOCecGp8bt6_OeF1ymKm09vgL1kT5LZu
"""

from google.colab import files

uploaded = files.upload()  #this uploads my csv file to collab

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os

excel_file_path = "/content/drive/My Drive/IT 244/text2sql_log.xlsx"

import sqlite3


#reads the CSV file into a pandas dataframe
df = pd.read_csv("all_seasons2.csv")

#this checks the data
df.head()

#creates a SQLite database file
conn = sqlite3.connect("nba_data.db")
cursor = conn.cursor()

#converts to an SQL table
df.to_sql("all_seasons2", conn, if_exists="replace", index=False)

print("Data loaded")

#testing to see if data is uploaded and able to be retrived
query = "SELECT player_name FROM all_seasons2 ORDER BY age DESC"
df_query = pd.read_sql_query(query, conn)

print(df_query)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers
#

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

#loads text2sql model and the tokenizer from huggingface
model_name = "gaussalgo/T5-LM-Large-text2sql-spider"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def generate_sql(natural_language_query):
    schema = """
    Table: all_seasons2
    Columns:
    - player_id (INTEGER)
    - player_name (TEXT)
    - team_abbreviation (TEXT)
    - age (INTEGER)
    - player_height (FLOAT)
    - player_weight (FLOAT)
    - college (TEXT)
    - country (TEXT)
    - draft_year (INTEGER)
    - draft_round (INTEGER)
    - pts (FLOAT)
    - reb (FLOAT)
    - ast (FLOAT)
    - net_rating (FLOAT)
    - oreb_pct (FLOAT)
    - dreb_pct (FLOAT)
    - usg_pct (FLOAT)
    - ts_pct (FLOAT)
    - ast_pct (FLOAT)
    - season (TEXT, e.g., '1996-97')

    Write a SQL query that accurately retrieves data from the all_seasons table using WHERE conditions as needed.
    - Use COUNT(*) when aggregating data.
    - Use ORDER BY COUNT(*) DESC when ranking grouped data.
    - Ensure all column names are spelled correctly.
    - When using LIMIT add an amount to the end e.g LIMIT 10.
    - Ensure using things like COUNT, are always capital.
    - Replace "along with" with ",".
    - Seperate data with ",".
    - DO NOT use "-" in the query.
    """


    #formatting input for the model
    input_text = f"### Schema: {schema} \n### Question: {natural_language_query} \n### SQL Query:"

    #converts input text into tokens for the model to understand
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

    #this generates the sql query
    with torch.no_grad():
        outputs = model.generate(**inputs)

    #turning the generated output into a sql query that is able to be exectuted
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return sql_query



def save_query_to_excel(nl_query, sql_query, file_path=excel_file_path):
    """Appends a new NL query and SQL query to an existing or new Excel file."""

    #checks to see if the csv file already exists
    if os.path.exists(file_path):
        #if the file exist it just adds to it
        df = pd.read_excel(file_path)
    else:
        #if it doesnt exist it creates it
        df = pd.DataFrame(columns=["Natural Language Query", "Generated SQL Query"])

    #adding a new row for next input and output to be logged
    new_data = pd.DataFrame([[nl_query, sql_query]], columns=df.columns)
    df = pd.concat([df, new_data], ignore_index=True)

    #saves to the excel file
    df.to_excel(file_path, index=False)

    print("Query has been logged")

# submiting a query
nl_query = "give me the player_name, team_abbreviation for all players"
sql_query = generate_sql(nl_query)
print("Generated SQL Query:", sql_query)

# logs the query
save_query_to_excel(nl_query, sql_query)

#function to execute the query
def execute_sql_query(sql_query, conn):

    try:
        df_result = pd.read_sql_query(sql_query, conn)
        return df_result
    except Exception as e:
        print("Error executing query:", e)
        return None

#checks to see if query worked, if it did it displays the results
df_results = execute_sql_query(sql_query, conn)

if df_results is not None:
    print("Query Results:")
    print(df_results)
else:
    print("No results returned.")